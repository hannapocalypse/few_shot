{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary libraries\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my implementation I am focussing on one aspect of the paper that was question-answering capability. For this I am using an existing deep learning framework (PyTorch) for my implementation. I am using a pre-trained LM as my backbone code, in this case the Huggng Face Transformers Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f9384c50e84986a7c7250bad216025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcab3c5b9de4da98719335b0064dfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d549aa175f404486fdbd6a1a335a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a1c33bb904938a8464f84acb47761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#setting up the environment\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2ForQuestionAnswering.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating my toy dataset\n",
    "\n",
    "toy_dataset = [\n",
    "    {\"context\": \"The capital of Germany is Luneburg.\", \"question\": \"What is the capital of Germany?\", \"answer\": \"Luneburg\"},\n",
    "    {\"context\": \"Luneburg is known for its Stintbridge.\", \"question\": \"What is a famous landmark in Luneburg?\", \"answer\": \"Stintbridge\"},\n",
    "    {\"context\": \"Leuphana is the best university in Luneburg.\", \"question\": \"What is the best university in Luneburg?\", \"answer\": \"Leuphana\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step where I implement few-shot learning I had help from ChatGPT to create this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Few-Shot Learning\n",
    "def few_shot_question_answering(contexts, question):\n",
    "    # Concatenate provided contexts into one string\n",
    "    context = \" \".join(contexts)\n",
    "    \n",
    "    # Encode context and question using the pre-trained model\n",
    "    inputs = tokenizer(context, question, return_tensors='pt')\n",
    "    \n",
    "    # Make predictions\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract answer from the output\n",
    "    start_idx = torch.argmax(outputs['start_logits'])\n",
    "    end_idx = torch.argmax(outputs['end_logits'])\n",
    "    answer = tokenizer.decode(inputs['input_ids'][0, start_idx:end_idx+1])\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I tried to get an answer to a question based on the small dataset I gave the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts: ['The capital of Germany is Luneburg.', 'Luneburg is known for its Stintbridge.', 'Leuphana is the best university in Luneburg.']\n",
      "Question: What is a characteristic of Luneburg?\n",
      "Predicted Answer: The capital of Germany is Luneburg. Luneburg is known for its Stintbridge. Leuphana is the best university in L\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate few-shot question answering ability\n",
    "question_about_LG = \"What is a characteristic of Luneburg?\"\n",
    "\n",
    "# Provide the model with a few examples related to Luneburg from the toy dataset\n",
    "example_contexts = [example['context'] for example in toy_dataset]\n",
    "\n",
    "# Apply few-shot learning\n",
    "predicted_answer = few_shot_question_answering(example_contexts, question_about_LG)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Contexts: {example_contexts}\")\n",
    "print(f\"Question: {question_about_LG}\")\n",
    "print(f\"Predicted Answer: {predicted_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is: It doesn't really work, as the model doesn't predict anything but seemingly just prints all information from the toy dataset. Not sure where I went wrong -> discuss in class?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
